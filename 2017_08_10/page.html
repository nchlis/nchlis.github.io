<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>
<h1 id="applying-dropout-in-convolutional-neural-nets-where-and-to-what-extent">Applying Dropout in Convolutional Neural Nets: Where and to what extent?</h1>
<p><a href="../index.html">Back to index</a></p>
<p><a href="https://github.com/nchlis/CNN_dropout">Source code for this project</a></p>
<h2 id="motivation-do-we-still-need-dropout">Motivation: Do we still need Dropout?</h2>
<h3 id="short-answer-yes.">Short answer: Yes.</h3>
<p>Convolutional Neural Networks (CNNs) are currently the state of the art method</p>
<p>when it comes to computer vision tasks. However, the datasets usually available</p>
<p>are not large <em>enough</em> so CNNs tend to overfit and not generalize as well to</p>
<p>new data. <a href="http://jmlr.org/papers/v15/srivastava14a.html">Dropout</a> is the standard method of regularizing neural networks</p>
<p>(including CNNs) and has been used extensively over the years. For example in</p>
<p><a href="https://arxiv.org/pdf/1409.1556.pdf">VGG</a> or <a href="https://arxiv.org/pdf/1502.01852.pdf">VGG-like networks</a> . Nevertheless, <strong>Dropout tends to increase the number</strong></p>
<p><strong>of epochs required until convergence</strong>.</p>
<p>As a result, lately there's an emerging trend (e.g. <a href="https://arxiv.org/abs/1512.00567">Inception v3</a> and <a href="https://arxiv.org/abs/1512.03385">Residual Networks</a>)</p>
<p>to only apply <a href="https://arxiv.org/abs/1502.03167">Batch Normalization</a> which also has a regularizing effect.</p>
<p>In the original Dropout paper it is demonstrated that it is beneficial to apply</p>
<p>Dropout to fully connected, as well as convolutional layers in a VGG-like network.</p>
<p>Nevertheless, in most cases where Dropout is used, it is usually applied</p>
<p><strong>only in the last fully connected layer(s)</strong> in <a href="https://arxiv.org/pdf/1409.1556.pdf">VGG</a>, <a href="https://arxiv.org/pdf/1502.01852.pdf">VGG-like networks</a></p>
<p>or other architectures like <a href="https://arxiv.org/abs/1610.02357">Xception</a>.</p>
<p>In <a href="https://arxiv.org/abs/1602.07261">Inception v4</a> Dropout is applied only to the last average pooling layer, since there are no</p>
<p>fully connected layers. One exception to the above trend is <a href="https://arxiv.org/abs/1605.07146">Wide ResNets</a>, where</p>
<p>it is demonstrated that applying dropout between convolutional layers in ResNets</p>
<p>is generally a good idea.</p>
<p>In this post we will demonstrate that <strong>using Dropout in conjunction with</strong></p>
<p><strong>Batch Normalization is beneficial</strong> even for simple VGG-like architectures.</p>
<p>We argue that even in cases where Batch Normalization can complement or possibly</p>
<p>be a subtitute for the the regularizing effect of Dropout, the additional</p>
<p><strong>ensembling effect of Dropout</strong> still leads to gains in generalization performance.</p>
<p>However, this comes at the <strong>cost of additional epochs</strong> being required during training.</p>
<h2 id="where-to-apply-dropout">Where to apply Dropout?</h2>
<h3 id="both-in-conv-and-fully-connected-layers-but-to-different-extent.">Both in Conv and Fully Connected layers, but to different extent.</h3>
<p>We will show that applying Dropout in convolutional layers can be tricky. To be</p>
<p>precise, if the dropout probability is too high, the overal performance of the</p>
<p>network deteriorates. But If the dropout probability in the convolutional layers</p>
<p>is small enough, there is an increase in performance. Moreover, we will study in</p>
<p>more detail the <strong>effect of dropout in convolutional layers of different depth</strong> in</p>
<p>in the network. Something that is hinted at, but not fully demonstrated in the</p>
<p>original Dropout paper.</p>
<h2 id="the-cifar-10-dataset">The CIFAR-10 dataset</h2>
<p>The <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR 10 dataset</a> consists of 60,000 images (50,000 training, 10,000 test set)</p>
<p>that belong to 10 distinct categories. Each image is 32x32x3. It's quite small</p>
<p>for today's standards but it is a nice dataset to play around with, since re-training</p>
<p>a different network on it is relatively fast (few hours). So it's perfect for experiments</p>
<p>if you are doing deep learning on a <em>less-than-infinite</em> budget. Next, we show</p>
<p>one image for each of the classes present in the dataset</p>
<p><img src="./CIFAR10_all_categories.png" alt="Drawing" style="width: 300px;"/></p>
<p>Just a reminder: the purpose of this post is to explore the properties of Dropout</p>
<p>in CNNs, not to reach state of the art results in CIFAR10. In this post we will use</p>
<p>the CIFAR10 <em>test set</em> as a <em>validation set</em> (to save the model that performs best</p>
<p>on the validation set during training). So to be technically correct, accuracy on</p>
<p>the CIFAR 10 test set, is validation accuracy (and not test accuracy) in this case.</p>
<h2 id="network-architecture">Network architecture</h2>
<p>We will use keras to define our networks. We will use a VGG-like convolutional</p>
<p>network that consists of 6 convolutional and 2 fully connected layers. The convolutional</p>
<p>layers are separated into 3 blocks of 2 layers each. Convolutional layers in the</p>
<p>same block have the same number of features. We apply Dropout and max-poooling after every</p>
<p>convolutional block. We apply Batch Normalization before every ReLU activation.</p>
<p>Finally, we apply Dropout after every Fully Connected (Dense) layer. We will use</p>
<p><a href="https://arxiv.org/abs/1412.6980">adam</a> to train the network, with default parameters as set defined by Keras</p>
<p>(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0). We will train</p>
<p>each network for 250 epochs and save the model with the best performance on the</p>
<p>validation set.</p>
<h3 id="the-newtork-in-detail">The newtork in detail</h3>
<p>Please note that a dropout layer with dropout probability = 0, is just an</p>
<p>identity layer. This allows us to re-use the same code to define networks that</p>
<p>do or do not have dropout at different points, just by changing the argument</p>
<p>of the dropout layer. Overall, the network has <strong>approximately 2M parameters</strong></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]<span class="co">#1 value for each conv block</span>
dropout_rate_dense <span class="op">=</span> <span class="fl">0.0</span>

model_id<span class="op">=</span><span class="st">&#39;CNN_bn_&#39;</span><span class="op">+</span><span class="bu">str</span>(add_BatchNorm)<span class="op">+</span><span class="st">&#39;_dropConv_&#39;</span><span class="op">+</span><span class="bu">str</span>(dropout_rate_conv[<span class="dv">0</span>])<span class="op">+</span><span class="st">&#39;_&#39;</span><span class="op">+\</span>
<span class="bu">str</span>(dropout_rate_conv[<span class="dv">1</span>])<span class="op">+</span><span class="st">&#39;_&#39;</span><span class="op">+</span><span class="bu">str</span>(dropout_rate_conv[<span class="dv">2</span>])<span class="op">+</span><span class="st">&#39;_&#39;</span><span class="op">+</span><span class="st">&#39;dropDense_&#39;</span><span class="op">+</span><span class="bu">str</span>(dropout_rate_dense)
<span class="bu">print</span>(<span class="st">&#39;Build model...&#39;</span>,model_id)

model <span class="op">=</span> Sequential()

<span class="co">#Conv block #1</span>
model.add(Conv2D(nfilters[<span class="dv">0</span>], (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>,
                 input_shape<span class="op">=</span>X_tr.shape[<span class="dv">1</span>:]))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_conv[<span class="dv">0</span>]))

model.add(Conv2D(nfilters[<span class="dv">0</span>], (<span class="dv">3</span>, <span class="dv">3</span>)))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_conv[<span class="dv">0</span>]))

model.add(MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)))

<span class="co">#Conv block #2</span>
model.add(Conv2D(nfilters[<span class="dv">1</span>], (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_conv[<span class="dv">1</span>]))

model.add(Conv2D(nfilters[<span class="dv">1</span>], (<span class="dv">3</span>, <span class="dv">3</span>)))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_conv[<span class="dv">1</span>]))

model.add(MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)))

<span class="co">#Conv block #3</span>
model.add(Conv2D(nfilters[<span class="dv">2</span>], (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_conv[<span class="dv">2</span>]))

model.add(Conv2D(nfilters[<span class="dv">2</span>], (<span class="dv">3</span>, <span class="dv">3</span>)))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_conv[<span class="dv">2</span>]))

model.add(MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)))

<span class="co">#at this point each image has shape (None, 2, 2, nfilters[2])</span>
model.add(Flatten())
<span class="co">#at this point each image has shape (None, 2*2*nfilters[2])</span>

model.add(Dense(ndense))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_dense))

model.add(Dense(ndense))
<span class="cf">if</span>(add_BatchNorm<span class="op">==</span><span class="va">True</span>):
    model.add(BatchNormalization(axis<span class="op">=-</span><span class="dv">1</span>))
model.add(Activation(<span class="st">&#39;relu&#39;</span>))
model.add(Dropout(dropout_rate_dense))

model.add(Dense(num_classes))
model.add(Activation(<span class="st">&#39;softmax&#39;</span>))

model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>,
              optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>,
              metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</code></pre></div>
<h2 id="results">Results</h2>
<p>For the remainder of this post we will consider 2 levels of Dropout, for simplicity:</p>
<ul>
<li><p><em>Low Dropout</em>: 0.2 probability to drop a unit.</p></li>
<li><p><em>High Dropout</em>: 0.5 probability to drop a unit.</p></li>
</ul>
<h2 id="establishing-a-baseline-with-batch-normalization">Establishing a baseline with Batch Normalization</h2>
<p>We will consider a batch normalized version of the network as the baseline for</p>
<p>all subsequent comparisons. Batch normalization leads to a considerable increase</p>
<p>in classification accuracy of the validation set, compared to ta <em>vanilla</em> version</p>
<p>of the network where no regularization is used.</p>
<p><img src="./baseline.png" alt="Drawing" style="width: 500px;"/></p>
<p><img src="./baseline_curve.png" alt="Drawing" style="width: 500px;"/></p>
<h4 id="note-batch-normalization-increased-execution-time-per-epoch-by-41">Note: Batch Normalization increased execution time per epoch by ~41%</h4>
<p>We can see that training the <em>vanilla</em> network deteriorates after ~150 epochs,</p>
<p>perhaps since no decay was used with adam. On the other hand, there are no problems</p>
<p>when training the batch normalized version of the network.</p>
<h2 id="dropout-in-conv-layers-low-is-beneficial-high-is-bad.">Dropout in Conv layers: Low is beneficial, High is bad.</h2>
<h4 id="low-dropout-increase-in-test-accuracy">Low Dropout: increase in test accuracy</h4>
<p>We do not apply dropout after the fully connected (Dense) layers. In the</p>
<p><strong>low dropout setting</strong> we setup the above network according to:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>]<span class="co">#low dropout of 0.2</span>
dropout_rate_dense <span class="op">=</span> <span class="fl">0.0</span></code></pre></div>
<p>As we can see below, applying batch normalization + low dropout increases classification</p>
<p>accuracy on the validation set, compared to the baseline (batch normalization, no dropout).</p>
<h4 id="high-dropout-training-the-network-is-probably-harder">High Dropout: training the network is (probably) harder</h4>
<p>In the <strong>high dropout setting</strong> we setup the network according to</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>]<span class="co">#high dropout of 0.5</span>
dropout_rate_dense <span class="op">=</span> <span class="fl">0.0</span></code></pre></div>
<p>If we increase the dropout probability to 0.5, the performance of the network</p>
<p>deteriorates.</p>
<p><img src="./dropout_conv.png" alt="Drawing" style="width: 500px;"/></p>
<p><img src="./dropout_conv_curve.png" alt="Drawing" style="width: 500px;"/></p>
<p>Deterioration of network performance with high dropout in the convolutional layers</p>
<p>may be attributed to one of two possible causes (or both).</p>
<p><strong>(1)</strong> High dropout of 0.5 leaves out half the network, so the CNN does not have enough</p>
<p>capacity to model the task at hand.</p>
<p><strong>(2)</strong> High dropout hinders training of the CNN.</p>
<p>We demonstrate that <strong>(1)</strong> is not the case, by training a network where we</p>
<p>use batch normalization, no dropout but <strong>halve the number of filters</strong> at each convolutional</p>
<p>layer.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">32</span>,<span class="dv">64</span>,<span class="dv">128</span>] <span class="co">#half number of filters</span>
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.0</span></code></pre></div>
<p>The smaller network performs much better than the large network with 0.5</p>
<p>dropout, so model capacity is not a problem. Therefore, we conclude that</p>
<p><strong>Using high dropout values in convolutional layers probably hinders training</strong>.</p>
<p><img src="./dropout_conv_halffilters.png" alt="Drawing" style="width: 500px;"/></p>
<p><img src="./dropout_conv_halffilters_curve.png" alt="Drawing" style="width: 500px;"/></p>
<h2 id="dropout-in-fully-connected-layers-the-more-the-merrier.">Dropout in Fully Connected layers: The more the merrier.</h2>
<p>In general, increases Dropout in Fully Connected (Dense) layers is a good idea,</p>
<p>since units of fully connected layers are much more reduntant than units of</p>
<p>convolutional layers. However, larger values of Dropout tend to require a larger</p>
<p>number of epochs until convergence (but the execution time per epoch is pretty much</p>
<p>the same). Last, one should be aware of the extreme case where Dropout is too</p>
<p>high and the network is underfitting.</p>
<p>In the <strong>low dropout setting</strong> we setup the above network according to:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.2</span><span class="co">#low dropout of 0.2</span></code></pre></div>
<p>In the <strong>high dropout setting</strong> we setup the network according to</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.5</span><span class="co">#high dropout of 0.5</span></code></pre></div>
<p>We can see than increases dropout in fully connected layers increases classification</p>
<p>accuracy on the test set, but <strong>the improvement is marginal</strong> compared to adding</p>
<p>dropout after the convolutional layers. One possible expanation for this</p>
<p>could be that in the above network, the <strong>dense layers correspond to ~0.8M</strong> parameters</p>
<p>out <strong>of the ~2M</strong> parameters of the network. So is the improvement marginal</p>
<p>because dense layers are responsible for a minority of the overall parameters</p>
<p>of the network? We will come to this later.</p>
<p><img src="./dropout_dense.png" alt="Drawing" style="width: 500px;"/></p>
<p><img src="./dropout_dense_curve.png" alt="Drawing" style="width: 500px;"/></p>
<h2 id="dropout-in-the-convolutional-and-fully-connected-layers-best-overall-performace">Dropout in the Convolutional and Fully Connected layers: best overall performace</h2>
<p>Now we combine what we learned above and apply dropout in both the convolutional</p>
<p>and fully connected parts of the network. We only use Dropout with probability 0.2</p>
<p>for the convolutional layers and we try out Dropout with probability 0.2 or 0.5</p>
<p>for both fully connected layers.</p>
<p>So the first network is:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.2</span><span class="co">#low dropout of 0.2</span></code></pre></div>
<p>while the second network is:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">512</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.5</span><span class="co">#high dropout of 0.5</span></code></pre></div>
<p>We can see that simply combining what performed best in our previous tests:</p>
<p>Dropout 0.2 in the convolutional and Dropout 0.5 in the fully connected parts,</p>
<p>led to the best result overall.</p>
<p><img src="./dropout_both.png" alt="Drawing" style="width: 500px;"/></p>
<p><img src="./dropout_both_curve.png" alt="Drawing" style="width: 500px;"/></p>
<h4 id="upscaling-the-fully-connected-layers">Upscaling the Fully Connected layers</h4>
<p>Previously we speculated whether the minimal increase in performance when applying</p>
<p>Dropout in the fully connected layers only can be attributed to the fact that</p>
<p>fully connected layers make up for ~<strong>0.8M</strong> of the ~<strong>2M</strong> parameters of the network.</p>
<p>Now we demonstrate that <strong>adding Dropout in convolutional layers is beneficial,</strong></p>
<p><strong>even if fully connected layers make up for the majority of model parameters. </strong></p>
<p>To be precise, we upscale both fully connected layers from 512 to 1024 units each,</p>
<p>while leaving the convolutional part of the network the same. Now the fully connected</p>
<p>layers make up for ~<strong>2M</strong> of the ~<strong>3.2M</strong> parameters of the network. Next, we</p>
<p>evaluate 3 different versions of the network</p>
<p>No Dropout, only Batch Normalization:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">1024</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.0</span></code></pre></div>
<p>Dropout in the fully connected layers only:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">1024</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.5</span><span class="co"># later 0.75</span></code></pre></div>
<p>Dropout in the convolutional &amp; fully connected layers:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nfilters <span class="op">=</span> [<span class="dv">64</span>,<span class="dv">128</span>,<span class="dv">256</span>]
ndense <span class="op">=</span> <span class="dv">1024</span>
add_BatchNorm <span class="op">=</span> <span class="va">True</span>
dropout_rate_conv <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>]
dropout_rate_dense <span class="op">=</span> <span class="fl">0.5</span><span class="co"># later 0.75</span></code></pre></div>
<p>Adding dropout to the fully connected layers marginally increased validation</p>
<p>accuracy, while also adding dropout in the convolutional layers had a more</p>
<p>pronounced positive effect. It is interesting however, that none of the new</p>
<p>configurations beat the previous best of 0.8831 validation accuracy.</p>
<p><img src="./dropout_3M.png" alt="Drawing" style="width: 500px;"/></p>
<p><img src="./dropout_3M_curve.png" alt="Drawing" style="width: 500px;"/></p>
<h2 id="applying-dropout-to-convolutional-blocks-of-different-depth">Applying Dropout to Convolutional Blocks of different depth</h2>
<p>Let's see what happens if we add <em>low</em> dropout of 0.2 and <em>high</em> dropout of 0.5</p>
<p>to <strong>only 1 of the 3 convolutional blocks</strong>.</p>
<p><em>Reminder:</em> The network consists of 3 convolutional blocks, with each block</p>
<p>consisting of 2 convolutional layers, while each convolution is followed by</p>
<p>Batch Normalization and a ReLU activation function. After each convolutional</p>
<p>block, max pooling is performed.</p>
<p><strong>Does the effect of Dropout depend on the depth of the convolutional block?</strong></p>
<p><strong>Yes!</strong> As we can see below, shallower convolutional blocks are more sensitive to</p>
<p>Dropout, so lower values are recommended. As we go deeper into the network,</p>
<p>higher values of Dropout can be used. For example, in the third convolutional block</p>
<p>there is an increase in performance even if we use <em>high</em> dropout (but <em>low</em> dropout</p>
<p>increases performance even more).</p>
<p><img src="./dropout_convblocks_02_05.png" alt="Drawing" style="width: 500px;"/></p>
<h2 id="code-availability">Code availability</h2>
<p>The source code of this project is <a href="https://github.com/nchlis/CNN_dropout">freely available on github</a>.</p>
<h2 id="additional-information-useful-links">Additional information &amp; Useful links</h2>
<ul>
<li><p><a href="https://www.youtube.com/watch?v=LxfUGhug-iQ">cs231n CNN lecture on youtube.</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=hd_KFJ5ktUc">cs231n lecture on youtube, covering Dropout, Batch Normaliztion and Adam.</a></p></li>
<li><p><a href="http://torch.ch/blog/2015/07/30/cifar.html">92.45% on CIFAR-10 in Torch</a></p></li>
<li><p><a href="https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py">Keras example on CIFAR classification</a></p></li>
<li><p><a href="http://jmlr.org/papers/v15/srivastava14a.html">paper - Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></p></li>
<li><p><a href="https://arxiv.org/abs/1502.03167">paper - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p></li>
<li><p><a href="https://arxiv.org/abs/1409.1556">paper - Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1502.01852.pdf">paper - Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p></li>
<li><p><a href="https://arxiv.org/abs/1512.00567">paper - Rethinking the Inception Architecture for Computer Vision</a></p></li>
<li><p><a href="https://arxiv.org/abs/1512.03385">paper - Deep Residual Learning for Image Recognition</a></p></li>
<li><p><a href="https://arxiv.org/abs/1602.07261">paper - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></p></li>
<li><p><a href="https://arxiv.org/abs/1610.02357">paper - Xception: Deep Learning with Depthwise Separable Convolutions</a></p></li>
<li><p><a href="https://arxiv.org/abs/1605.07146">paper - Wide Residual Networks</a></p></li>
</ul>
</body>
</html>
